{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from rank_metrics import mean_reciprocal_rank, mean_average_precision, ndcg_at_k\n",
    "from utils import import_data\n",
    "\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 881052 lines from 31 files\n"
     ]
    }
   ],
   "source": [
    "train = import_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algolia Search performance metrics\n",
    "\n",
    "This section evaluate Algolia Search performance when a click is performed.\n",
    "\n",
    "Metrics functions are coming from https://gist.github.com/bwhite/3726239.\n",
    "Due to time constraints, these metrics have not been formely tested ðŸ˜•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a click matrix to compute perf metrics\n",
    "clicks_matrix = []\n",
    "for index, row in train.iterrows():\n",
    "    if isinstance(row['clicks'], list) and row['clicks']:\n",
    "        if row['nb_hits']:\n",
    "            try:\n",
    "                _array = np.zeros(row['nb_hits'])\n",
    "                for click in row['clicks']:\n",
    "                    _array[click['position'] - 1] = 1\n",
    "                clicks_matrix.append(_array)\n",
    "            except:\n",
    "                pass # Some case are not working => parsing errors ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean reciprocal rank\n",
    "https://en.wikipedia.org/wiki/Mean_reciprocal_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5335751463744777"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reciprocal_rank(clicks_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Average Precision\n",
    "https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5168622744145244"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(clicks_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algolia search performance metrics for learning to rank experiment\n",
    "These metrics are computed on the same dataframe as Ml experiments in order to have comparative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nb_clicks'] = train['clicks'].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0)\n",
    "train = train[train['nb_clicks'] > 0]\n",
    "\n",
    "train['nb_hits_displayed'] = train['hits'].apply(\n",
    "    lambda x: len(x) if isinstance(x, list) else 0)\n",
    "train = train[train['nb_hits_displayed'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a click matrix to compute perf metrics\n",
    "clicks_matrix = []\n",
    "for index, row in train.iterrows():\n",
    "    if isinstance(row['clicks'], list) and row['clicks']:\n",
    "        if row['nb_hits']:\n",
    "            try:\n",
    "                _array = np.zeros(row['nb_hits'])\n",
    "                for click in row['clicks']:\n",
    "                    _array[click['position'] - 1] = 1\n",
    "                clicks_matrix.append(_array)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53245333868448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reciprocal_rank(clicks_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision(clicks_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG @ 1, 3, 5 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 3, 5, 10]:\n",
    "    ndcg_ = []\n",
    "    for query in clicks_matrix:\n",
    "        ndcg_.append(ndcg_at_k(query, i))\n",
    "    print('> nDCG@{} is : {}'.format(i, pd.Series(ndcg_).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics comparison with ML experiments\n",
    "This is a simple plot to compare Learning-to-rank metrics on algolia search and proposed ML experiments, metrics are coming from `rank_xgboost.py` and `rank_tensorflow.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'NDCG@':[1, 3, 5, 10],\n",
    "                   'Algolia':[0.3703, 0.5405, 0.5940, 0.6517],\n",
    "                   'Tensorflow':[0.4291, 0.5616, 0.6192, 0.6914],\n",
    "                   'Xgboost':[0.4643, 0.5874, 0.6247, 0.6469]})\n",
    "df = df.melt('NDCG@', var_name='Algorithm',  value_name='NDCG')\n",
    "g = sns.factorplot(x=\"NDCG@\", y=\"NDCG\", hue='Algorithm', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
